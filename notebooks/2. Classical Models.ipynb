{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML\n",
    "### Notebook Deprecated\n",
    "Deprecated, classical models don't seem to be working very well (because of lack of context, will fix this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud  \n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = \"../data\"\n",
    "DATA_NAME = \"BC5CDR-chem\"\n",
    "DATA_DIR = path(DB_DIR, DATA_NAME)\n",
    "TRAIN = \"train.tsv\"\n",
    "DEV = \"devel.tsv\"\n",
    "TEST = \"test.tsv\"\n",
    "\n",
    "train_data = read_data(path(DATA_DIR, TRAIN))\n",
    "dev_data = read_data(path(DATA_DIR, DEV))\n",
    "test_data = read_data(path(DATA_DIR, TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM on statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2f(sent, i):\n",
    "    \"\"\"naive feature engineering. Context is only +-1 word\"\"\"\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'word.len()' : len(word),\n",
    "        'word.isdigit()' : word.isdigit(),\n",
    "        'word.isupper()' : word.isupper(),\n",
    "        'word.islower()' : word.islower(),\n",
    "        'word.istitle()' : word.istitle(),\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "        'len(word-1)' : len(word1),\n",
    "        'word-1.isdigit()' : word1.isdigit(),\n",
    "        'word-1.isupper()' : word1.isupper(),\n",
    "        'word-1.islower()' : word1.islower(),\n",
    "        'word-1.istitle()' : word1.istitle(),\n",
    "    })\n",
    "    else:\n",
    "        features.update({\n",
    "        'len(word-1)' : -1,\n",
    "        'word-1.isdigit()' : -1,\n",
    "        'word-1.isupper()' : -1,\n",
    "        'word-1.islower()' : -1,\n",
    "        'word-1.istitle()' : -1,\n",
    "    })\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "        'len(word+1)' : len(word1),\n",
    "        'word+1.isdigit()' : word1.isdigit(),\n",
    "        'word+1.isupper()' : word1.isupper(),\n",
    "        'word+1.islower()' : word1.islower(),\n",
    "        'word+1.istitle()' : word1.istitle(),\n",
    "    })\n",
    "    else:\n",
    "        features.update({\n",
    "        'len(word+1)' : -1,\n",
    "        'word+1.isdigit()' : -1,\n",
    "        'word+1.isupper()' : -1,\n",
    "        'word+1.islower()' : -1,\n",
    "        'word+1.istitle()' : -1,\n",
    "    })\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    X, y = [], []\n",
    "    for instance in data:\n",
    "        labels, text = instance[0].split(), instance[1].split()\n",
    "        X += [w2f(text, i) for i in range(len(text))]\n",
    "        y += labels\n",
    "    X = pd.DataFrame(X)\n",
    "    assert X.shape[0] == len(y), \"Failure\"\n",
    "    return pd.DataFrame(X), y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(train_data)\n",
    "X_dev, y_dev = prepare_data(dev_data)\n",
    "X_test, y_test = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical labels: ['B' 'I' 'O']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "print(f\"Encoding categorical labels: {le.classes_}\")\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_dev = le.transform(y_dev)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = X_train[col].astype(int)\n",
    "    \n",
    "for col in X_dev.columns:\n",
    "    X_dev[col] = X_dev[col].astype(int)\n",
    "    \n",
    "for col in X_test.columns:\n",
    "    X_test[col] = X_test[col].astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\"objective\": \"multiclass\",\n",
    "          \"num_class\": 3,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.1,\n",
    "          'max_depth': 5,\n",
    "          \"num_leaves\": 5,\n",
    "          \"max_bin\": 32,\n",
    "          \"feature_fraction\": 0.7,\n",
    "          \"verbosity\": 0,\n",
    "          \"drop_rate\": 0.1,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_sum_hessian_in_leaf\": 10,\n",
    "          \"bagging_fraction\": 0.7,\n",
    "          \"bagging_freq\": 5,\n",
    "          \"random_seed\" : 123123123,\n",
    "        }\n",
    "\n",
    "num_boost_round = int(1e5)\n",
    "early_stopping_rounds = 500\n",
    "verbose_eval = 300\n",
    "\n",
    "train_dataset = lgb.Dataset(X_train, y_train)\n",
    "dev_dataset = lgb.Dataset(X_dev, y_dev)\n",
    "test_dataset = lgb.Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[300]\ttraining's multi_logloss: 0.177375\tvalid_1's multi_logloss: 0.182315\n",
      "[600]\ttraining's multi_logloss: 0.171879\tvalid_1's multi_logloss: 0.179869\n",
      "[900]\ttraining's multi_logloss: 0.168674\tvalid_1's multi_logloss: 0.178618\n",
      "[1200]\ttraining's multi_logloss: 0.16655\tvalid_1's multi_logloss: 0.178228\n",
      "[1500]\ttraining's multi_logloss: 0.165163\tvalid_1's multi_logloss: 0.178041\n",
      "[1800]\ttraining's multi_logloss: 0.16405\tvalid_1's multi_logloss: 0.177898\n",
      "[2100]\ttraining's multi_logloss: 0.163098\tvalid_1's multi_logloss: 0.177749\n",
      "[2400]\ttraining's multi_logloss: 0.162302\tvalid_1's multi_logloss: 0.177808\n",
      "Early stopping, best iteration is:\n",
      "[2144]\ttraining's multi_logloss: 0.162969\tvalid_1's multi_logloss: 0.177692\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "            params, \n",
    "            train_dataset, \n",
    "            num_boost_round=num_boost_round,\n",
    "            valid_sets=[train_dataset, dev_dataset],\n",
    "            verbose_eval=verbose_eval,\n",
    "            early_stopping_rounds=early_stopping_rounds\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_pred = model.predict(X_dev)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.12      0.19      5347\n",
      "           1       0.66      0.14      0.24      1748\n",
      "           2       0.95      0.99      0.97    110358\n",
      "\n",
      "    accuracy                           0.94    117453\n",
      "   macro avg       0.72      0.42      0.47    117453\n",
      "weighted avg       0.93      0.94      0.92    117453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_dev, y_dev_pred.argmax(axis=1))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(sentence, model):\n",
    "    sentence = sentence.split()\n",
    "    X = [w2f(sentence, i) for i in range(len(sentence))]\n",
    "    X = pd.DataFrame(X)\n",
    "    for col in X.columns:\n",
    "        X[col] = X[col].astype(int)\n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"She suffers diabetes.\"\n",
    "out = infer(sentence, model).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2f(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()' : word.lower(),\n",
    "        'word[-3:]' : word[-3:],\n",
    "        'word[-2:]' : word[-2:],\n",
    "        'word.isupper()' : word.isupper(),\n",
    "        'word.islower()' : word.islower(),\n",
    "        'word.istitle()' : word.istitle(),\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "        'word-1.lower()' : word1.lower(),\n",
    "        'word-1.isupper()' : word1.isupper(),\n",
    "        'word-1.islower()' : word1.islower(),\n",
    "        'word-1.istitle()' : word1.istitle(),\n",
    "    })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "        'word+1.lower()' : word1.lower(),\n",
    "        'word+1.isupper()' : word1.isupper(),\n",
    "        'word+1.islower()' : word1.islower(),\n",
    "        'word+1.istitle()' : word1.istitle(),\n",
    "    })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    X, y = [], []\n",
    "    for instance in data:\n",
    "        labels, text = instance[0].split(), instance[1].split()\n",
    "        X.append([w2f(text, i) for i in range(len(text))])\n",
    "        y.append(labels)\n",
    "    return X, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(train_data)\n",
    "X_dev, y_dev = prepare_data(dev_data)\n",
    "X_test, y_test = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF, scorers, metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=10,\n",
    "          c2=0.1,\n",
    "          max_iterations=1000,\n",
    "          all_possible_transitions=False,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_pred = model.predict(X_dev)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_dev, y_dev_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
